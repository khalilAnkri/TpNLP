{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8841,"sourceType":"datasetVersion","datasetId":4133}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndirectory = '/kaggle/input/customer-support-on-twitter'\nfiles = os.listdir(directory)\nprint(files)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-23T20:00:58.513415Z","iopub.execute_input":"2024-02-23T20:00:58.513720Z","iopub.status.idle":"2024-02-23T20:00:58.519985Z","shell.execute_reply.started":"2024-02-23T20:00:58.513697Z","shell.execute_reply":"2024-02-23T20:00:58.519057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/customer-support-on-twitter/sample.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.521966Z","iopub.execute_input":"2024-02-23T20:00:58.522253Z","iopub.status.idle":"2024-02-23T20:00:58.537100Z","shell.execute_reply.started":"2024-02-23T20:00:58.522226Z","shell.execute_reply":"2024-02-23T20:00:58.536392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.538065Z","iopub.execute_input":"2024-02-23T20:00:58.538365Z","iopub.status.idle":"2024-02-23T20:00:58.552208Z","shell.execute_reply.started":"2024-02-23T20:00:58.538321Z","shell.execute_reply":"2024-02-23T20:00:58.551581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Mise en minuscules**","metadata":{}},{"cell_type":"code","source":"data[\"text_lower\"] = data[\"text\"].str.lower()\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.553004Z","iopub.execute_input":"2024-02-23T20:00:58.553229Z","iopub.status.idle":"2024-02-23T20:00:58.569092Z","shell.execute_reply.started":"2024-02-23T20:00:58.553210Z","shell.execute_reply":"2024-02-23T20:00:58.568190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Suppression des ponctuations****","metadata":{}},{"cell_type":"code","source":"import string\n\n# Define the punctuation to remove\npunc_to_remove = string.punctuation\n\n# Function to remove punctuation from text\ndef remove_punc(text):\n    return text.translate(str.maketrans('', '', punc_to_remove))\n\n# Apply the remove_punc function to the 'text' column and store the result in a new column 'punc_remove'\ndata['punc_remove'] = data['text'].apply(remove_punc)\n\n\n# Display the first few rows of the dataframe\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.571046Z","iopub.execute_input":"2024-02-23T20:00:58.571248Z","iopub.status.idle":"2024-02-23T20:00:58.587088Z","shell.execute_reply.started":"2024-02-23T20:00:58.571230Z","shell.execute_reply":"2024-02-23T20:00:58.586154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suppression des mots vides**","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Get the set of English stopwords\nstopwords_set = set(stopwords.words('english'))\n\n# Function to remove stopwords from text\ndef remove_stopwords(text):\n    # Split the text into words and filter out stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word not in stopwords_set]\n    # Join the filtered words back into a string\n    return ' '.join(filtered_words)\n\n# Apply remove_stopwords function to the 'punc_remove' column and store the result in a new column 'stopwords_remove'\ndata['stopwords_remove'] = data['punc_remove'].apply(remove_stopwords)\n\n# Display the first 5 rows of the dataframe\ndata.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.588073Z","iopub.execute_input":"2024-02-23T20:00:58.588273Z","iopub.status.idle":"2024-02-23T20:00:58.608290Z","shell.execute_reply.started":"2024-02-23T20:00:58.588255Z","shell.execute_reply":"2024-02-23T20:00:58.607187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suppression des mots frÃ©quents**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Create a Counter object\nword_counter = Counter()\n\n# Loop through each text in the 'stopwords_remove' column\nfor text in data['stopwords_remove'].values:\n    # Split the text into words and update the counter\n    for word in text.split():\n        word_counter[word] += 1\n\n# Get the 10 most common words and their counts\nmost_common_words = word_counter.most_common(10)\n\nprint(most_common_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.609840Z","iopub.execute_input":"2024-02-23T20:00:58.610186Z","iopub.status.idle":"2024-02-23T20:00:58.622538Z","shell.execute_reply.started":"2024-02-23T20:00:58.610155Z","shell.execute_reply":"2024-02-23T20:00:58.621779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suppression des mots rares**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# Create a Counter object to count word occurrences\nword_counter = Counter()\n\n# Loop through each text in the 'stopwords_remove' column\nfor text in data['stopwords_remove'].values:\n    # Split the text into words and update the counter\n    for word in text.split():\n        word_counter[word] += 1\n\n# Define a threshold for word frequency (adjust as needed)\nfrequency_threshold = 5\n\n# Filter out words that occur less frequently than the threshold\ncommon_words = {word for word, count in word_counter.items() if count >= frequency_threshold}\n\n# Function to remove rare words from text\ndef remove_rare_words(text):\n    # Split the text into words and filter out rare words\n    words = text.split()\n    filtered_words = [word for word in words if word in common_words]\n    # Join the filtered words back into a string\n    return ' '.join(filtered_words)\n\n# Apply remove_rare_words function to the 'stopwords_remove' column and store the result in a new column 'common_words_remove'\ndata['common_words_remove'] = data['stopwords_remove'].apply(remove_rare_words)\n\n# Display the first 5 rows of the dataframe\ndata.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.624903Z","iopub.execute_input":"2024-02-23T20:00:58.625492Z","iopub.status.idle":"2024-02-23T20:00:58.648628Z","shell.execute_reply.started":"2024-02-23T20:00:58.625461Z","shell.execute_reply":"2024-02-23T20:00:58.647795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stemming**","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\n# Create a PorterStemmer object\nstemmer = PorterStemmer()\n\n# Function to stem words in text\ndef stem_words(text):\n    # Split the text into words, stem each word, and join them back into a string\n    return ' '.join([stemmer.stem(word) for word in text.split()])\n\n# Apply the stem_words function to the 'text' column and store the result in a new column 'stemmer_text'\ndata['stemmer_text'] = data['text'].apply(stem_words)\n\n# Display the first 2 rows of the dataframe\ndata.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.655081Z","iopub.execute_input":"2024-02-23T20:00:58.655604Z","iopub.status.idle":"2024-02-23T20:00:58.709687Z","shell.execute_reply.started":"2024-02-23T20:00:58.655575Z","shell.execute_reply":"2024-02-23T20:00:58.708812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatisation**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer= WordNetLemmatizer()\n\ndef lemmatize_words(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:00:58.710982Z","iopub.execute_input":"2024-02-23T20:00:58.711249Z","iopub.status.idle":"2024-02-23T20:01:18.748258Z","shell.execute_reply.started":"2024-02-23T20:00:58.711224Z","shell.execute_reply":"2024-02-23T20:01:18.747423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suppression des Ã©mojis , Suppression des Ã©moticÃ´nes**","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\nprint(remove_emoji(\"GI4 is on ðŸ”¥ðŸ”¥\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:06:44.699261Z","iopub.execute_input":"2024-02-23T20:06:44.699609Z","iopub.status.idle":"2024-02-23T20:06:44.705450Z","shell.execute_reply.started":"2024-02-23T20:06:44.699583Z","shell.execute_reply":"2024-02-23T20:06:44.704683Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"GI4 is on \n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Suppression des URL, Suppression des balises**","metadata":{}},{"cell_type":"code","source":"import re\n\n# Function to remove URLs from text\ndef remove_urls(text):\n    # Define the pattern for URLs\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    # Remove URLs using the pattern\n    return url_pattern.sub(r'', text)\n\n# Function to remove HTML tags from text\ndef remove_html_tags(text):\n    # Define the pattern for HTML tags\n    html_pattern = re.compile(r'<.*?>')\n    # Remove HTML tags using the pattern\n    return html_pattern.sub(r'', text)\n\n# Example usage:\ntext_with_urls_html = \"Check out my website at https://example.com! <p>This is a paragraph.</p>\"\ntext_without_urls = remove_urls(text_with_urls_html)\ntext_without_urls_html = remove_html_tags(text_without_urls)\nprint(text_without_urls_html)  \n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T20:04:45.337171Z","iopub.execute_input":"2024-02-23T20:04:45.337485Z","iopub.status.idle":"2024-02-23T20:04:45.343433Z","shell.execute_reply.started":"2024-02-23T20:04:45.337464Z","shell.execute_reply":"2024-02-23T20:04:45.342598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}