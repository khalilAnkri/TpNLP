{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANKRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANKRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      films adapted from comic books have had plenty...   \n",
      "1      for starters , it was created by alan moore ( ...   \n",
      "2      to say moore and campbell thoroughly researche...   \n",
      "3      the book ( or \" graphic novel , \" if you will ...   \n",
      "4      in other words , don't dismiss this film becau...   \n",
      "...                                                  ...   \n",
      "64715  that lack of inspiration can be traced back to...   \n",
      "64716  like too many of the skits on the current inca...   \n",
      "64717  after watching one of the \" roxbury \" skits on...   \n",
      "64718   bump unsuspecting women , and . . . that's all .   \n",
      "64719  after watching _a_night_at_the_roxbury_ , you'...   \n",
      "\n",
      "                                       Preprocessed_Text  \n",
      "0      films adapted comic books plenty success wheth...  \n",
      "1      starters created alan moore eddie campbell bro...  \n",
      "2      say moore campbell thoroughly researched subje...  \n",
      "3      book `` graphic novel `` 500 pages long includ...  \n",
      "4                          words n't dismiss film source  \n",
      "...                                                  ...  \n",
      "64715    lack inspiration traced back insipid characters  \n",
      "64716  like many skits current incarnation _saturday_...  \n",
      "64717  watching one `` roxbury `` skits snl come away...  \n",
      "64718                         bump unsuspecting women 's  \n",
      "64719  watching _a_night_at_the_roxbury_ 'll left exa...  \n",
      "\n",
      "[64720 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv('movie_review.csv') \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in punctuation]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "df['Preprocessed_Text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(df[['text', 'Preprocessed_Text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement du modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur du mot 'film': [-1.48878694e+00  3.40189397e-01 -2.80833364e-01 -8.27568054e-01\n",
      "  4.15798098e-01 -3.71084750e-01  1.73919126e-01  1.21929193e+00\n",
      " -3.83469522e-01  7.15771854e-01 -1.45414436e+00 -7.57309318e-01\n",
      "  7.68148303e-01  2.18357235e-01  1.05962694e+00 -1.20602405e+00\n",
      "  7.07985044e-01 -8.04454647e-03 -5.59647977e-01 -1.78992021e+00\n",
      "  7.77863503e-01 -8.48565042e-01  2.96196312e-01 -1.59255970e+00\n",
      " -1.20852746e-01  2.44949564e-01  1.87826559e-01  1.39504099e+00\n",
      " -9.47871029e-01  8.10221136e-01  1.76725781e+00 -3.31459641e-01\n",
      " -7.51761436e-01 -9.79740202e-01 -1.31908870e+00  1.73563138e-01\n",
      " -6.92288697e-01  1.33531049e-01 -4.32241172e-01 -2.53047466e-01\n",
      " -1.36244988e+00  5.27096748e-01 -1.65141809e+00  2.17090917e+00\n",
      " -4.47136045e-01 -3.54130536e-01  1.05105722e+00  2.46144589e-02\n",
      "  4.42524761e-01 -1.18203796e-01  1.60176426e-01  5.90398386e-02\n",
      " -3.33753973e-01 -1.27642155e+00 -1.15117931e+00 -4.21303838e-01\n",
      " -1.21318316e+00 -1.15292788e+00 -1.63571680e+00 -1.02178192e+00\n",
      "  1.22025895e+00 -1.14807582e+00  7.23125815e-01 -2.57938623e-01\n",
      " -1.54273319e+00  1.60006499e+00  1.33097053e+00  1.50413573e+00\n",
      " -2.29838514e+00  1.38415909e+00 -7.97392845e-01  1.88711715e+00\n",
      "  1.44020212e+00 -2.98755348e-01  9.68526065e-01  1.53846753e+00\n",
      " -7.20441461e-01  7.37165987e-01 -1.12865019e+00  4.33946460e-01\n",
      " -6.58048987e-01 -1.17029846e+00 -1.07633972e+00  4.20482278e-01\n",
      " -1.16017413e+00 -3.10716212e-01  7.66765833e-01  7.43276847e-04\n",
      "  2.03033775e-01  2.11301856e-02  1.85723197e+00  1.16683841e+00\n",
      "  1.08714318e+00 -1.16625130e+00  7.11893737e-01 -2.50388235e-01\n",
      "  7.79897392e-01  8.46675932e-01  1.00985363e-01 -1.06733298e+00]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "tokenized_texts = [text.split() for text in df['Preprocessed_Text']]\n",
    "\n",
    "# Paramètres du modèle Word2Vec\n",
    "vector_size = 100  # Taille des vecteurs de mots\n",
    "window = 5  # Taille de la fenêtre contextuelle\n",
    "min_count = 1  # Ignorer les mots ayant une fréquence inférieure à ce seuil\n",
    "sg = 0  # Utiliser l'algorithme Skip-gram (sg=1 pour CBOW)\n",
    "epochs = 10  # Nombre d'itérations sur l'ensemble des données\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_texts,\n",
    "                 vector_size=vector_size,\n",
    "                 window=window,\n",
    "                 min_count=min_count,\n",
    "                 sg=sg,\n",
    "                 epochs=epochs)\n",
    "\n",
    "\n",
    "print(\"Vecteur du mot 'film':\", model.wv['film'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation des reviews de movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Preprocessed_Text  \\\n",
      "0      films adapted comic books plenty success wheth...   \n",
      "1      starters created alan moore eddie campbell bro...   \n",
      "2      say moore campbell thoroughly researched subje...   \n",
      "3      book `` graphic novel `` 500 pages long includ...   \n",
      "4                          words n't dismiss film source   \n",
      "...                                                  ...   \n",
      "64715    lack inspiration traced back insipid characters   \n",
      "64716  like many skits current incarnation _saturday_...   \n",
      "64717  watching one `` roxbury `` skits snl come away...   \n",
      "64718                         bump unsuspecting women 's   \n",
      "64719  watching _a_night_at_the_roxbury_ 'll left exa...   \n",
      "\n",
      "                                           Review_Vector  \n",
      "0      [-0.49545103, 0.5904012, 0.38407725, 0.0540663...  \n",
      "1      [-0.22736661, 0.46170366, 0.2058562, 0.5463130...  \n",
      "2      [0.00074618176, 0.7641943, 0.6029967, 0.191977...  \n",
      "3      [-0.4881899, 0.45736974, 0.5530963, 0.51612663...  \n",
      "4      [-0.7714188, 0.86675644, 0.28849864, -0.327250...  \n",
      "...                                                  ...  \n",
      "64715  [0.2942758, 0.43034908, 0.21630363, -0.1116090...  \n",
      "64716  [-0.07516204, 0.58110046, 0.50812, 0.13637446,...  \n",
      "64717  [-0.24301217, 0.8000167, 1.1054308, 0.14138964...  \n",
      "64718  [0.030281678, 0.26852927, 0.53509945, -0.02614...  \n",
      "64719  [-0.66540766, 0.41898268, 0.32888472, -0.34327...  \n",
      "\n",
      "[64720 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_review_vector(review, model, vector_size):\n",
    "    \n",
    "    words = [word for word in review.split() if word in model.wv.key_to_index]\n",
    "    if not words:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[word] for word in words]\n",
    "    review_vector = np.mean(word_vectors, axis=0)\n",
    "    return review_vector\n",
    "\n",
    "df['Review_Vector'] = df['Preprocessed_Text'].apply(lambda x: get_review_vector(x, model, vector_size))\n",
    "\n",
    "print(df[['Preprocessed_Text', 'Review_Vector']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement : 51776\n",
      "Taille de l'ensemble de test : 12944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction d'un classificateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos' 'neg' ... 'pos' 'pos' 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train = np.vstack(train_data['Review_Vector'].values)\n",
    "y_train = train_data['tag']\n",
    "X_test = np.vstack(test_data['Review_Vector'].values)\n",
    "y_test = test_data['tag']\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5857540173053152\n",
      "Precision : 0.5830931796349664\n",
      "Recall : 0.6464323748668797\n",
      "F1-score : 0.6131313131313131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label='pos')\n",
    "recall = recall_score(y_test, y_pred, pos_label='pos')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "\n",
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"Precision :\", precision)\n",
    "print(\"Recall :\", recall)\n",
    "print(\"F1-score :\", f1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
